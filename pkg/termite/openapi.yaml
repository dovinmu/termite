openapi: "3.0.3"
info:
  version: v1alpha1
  title: Termite API
  description: |
    Termite is an Ollama-like local inference server for ONNX-based ML models.

    ## What is Termite

    Termite provides local ML inference with an Ollama-compatible API:
    - **Embedding Generation**: Text and multimodal (CLIP) embedding models
    - **Text Chunking**: Semantic chunking with ONNX models or fixed-size fallback
    - **Reranking**: Relevance re-scoring for search results
    - **Named Entity Recognition**: Extract persons, organizations, locations from text
    - **Text Rewriting**: Transform text using Seq2Seq models (question generation, query generation, etc.)

    Download the latest release at https://antfly.io/docs/downloads

    ## When to Use Termite

    Termite can run standalone or as part of an Antfly cluster:
    - Local ONNX model inference without external API dependencies
    - Ollama-compatible `/api/embed` endpoint for embeddings
    - Semantic text chunking for RAG pipelines
    - Relevance reranking for improved search quality
    - Centralized model serving across distributed nodes
    - Privacy-preserving ML inference (data never leaves your infrastructure)

    ## Features

    ### Embedding Generation
    - **Models**: ONNX models auto-discovered from `{models_dir}/embedders/`
    - **API**: Ollama-compatible `/api/embed` endpoint
    - **Response Formats**: Binary (default), JSON

    ### Multimodal Support (CLIP)
    - **Image Embeddings**: CLIP models for joint text-image embedding space
    - **Input Formats**: Base64 data URIs (`data:image/png;base64,...`) or URLs
    - **OpenAI-Compatible**: Uses content parts format (`{"type": "image_url", "image_url": {"url": "..."}}`)
    - **Use Cases**: Image search, cross-modal retrieval, visual similarity

    ### Text Chunking
    - **Models**: Fixed-size chunking (always available) + ONNX models
    - **Model Discovery**: Auto-discovers models from `{models_dir}/chunkers/`
    - **Caching**: 2-minute TTL memory cache
    - **Fallback**: Falls back to fixed chunking if model fails

    ### Reranking
    - **Model Discovery**: Auto-discovers ONNX models from `{models_dir}/rerankers/`
    - **Quantization**: Automatically uses quantized models if available
    - **Input**: Pre-rendered text prompts (client handles field extraction)

  contact:
    name: Antfly
    url: https://github.com/antflydb/termite
  license:
    name: All Rights Reserved
    url: https://antfly.io/legal
servers:
  - url: /api
    description: Termite API server

security: []  # No authentication required - Termite runs locally

components:
  schemas:
    Error:
      type: object
      required:
        - error
      properties:
        error:
          type: string
          description: Error message

    # Multimodal Content Types (OpenAI-compatible)
    TextContentPart:
      type: object
      description: Text content for embedding
      required:
        - type
        - text
      properties:
        type:
          type: string
          enum: [text]
        text:
          type: string
          description: Text content to embed

    ImageURL:
      type: object
      description: Image URL or data URI
      required:
        - url
      properties:
        url:
          type: string
          description: URL or data URI (data:image/png;base64,...)
          example: "data:image/png;base64,iVBORw0KGgo..."

    ImageURLContentPart:
      type: object
      description: Image content for embedding (OpenAI-compatible format)
      required:
        - type
        - image_url
      properties:
        type:
          type: string
          enum: [image_url]
        image_url:
          $ref: "#/components/schemas/ImageURL"

    ContentPart:
      description: A content part for multimodal embedding (text or image)
      oneOf:
        - $ref: "#/components/schemas/TextContentPart"
        - $ref: "#/components/schemas/ImageURLContentPart"

    # Embedding Types (Ollama-compatible with multimodal extension)
    EmbedRequest:
      type: object
      required:
        - model
        - input
      properties:
        model:
          type: string
          description: Name of the embedder model from models_dir/embedders/
          example: "bge-small-en-v1.5"
        input:
          oneOf:
            - type: string
              description: Single text string (backward compatible)
            - type: array
              items:
                type: string
              description: Array of text strings (backward compatible)
            - type: array
              items:
                $ref: "#/components/schemas/ContentPart"
              description: Array of multimodal content parts (text or images)
          description: |
            Input content to embed. Supports three formats:
            - Single text string: `"hello world"`
            - Array of text strings: `["hello", "world"]`
            - Array of content parts (multimodal): `[{"type": "text", "text": "hello"}, {"type": "image_url", "image_url": {"url": "data:image/png;base64,..."}}]`
          example: ["hello world", "machine learning"]
        truncate:
          type: boolean
          default: true
          description: Truncate input to fit model context length

    EmbedResponse:
      type: object
      example:
        {
          "model": "bge-small-en-v1.5",
          "embeddings": [[0.0123, -0.0456, 0.0789], [0.0234, -0.0567, 0.0890]],
        }
      required:
        - model
        - embeddings
      properties:
        model:
          type: string
          description: Model used for embedding
          example: "bge-small-en-v1.5"
        embeddings:
          type: array
          items:
            type: array
            items:
              type: number
              format: float
          description: Array of embedding vectors (one per input string)

    # Chunking Types - reference existing schemas
    Chunk:
      $ref: "../../../antfly-go/libaf/chunking/openapi.yaml#/components/schemas/Chunk"

    ChunkConfig:
      type: object
      description: |
        Configuration for chunking requests to Termite API.
        This is a simplified config for the HTTP API - differs from the full ChunkerConfig
        which includes provider selection and caching configuration.
      properties:
        model:
          type: string
          description: "The chunking model to use. Either 'fixed' for simple token-based chunking, or a model name from models/chunkers/{name}/."
          default: "fixed"
          example: "fixed"
        target_tokens:
          type: integer
          description: Target number of tokens per chunk
          example: 500
          default: 500
        overlap_tokens:
          type: integer
          description: Number of overlapping tokens between chunks
          example: 50
          default: 50
        separator:
          type: string
          description: Text separator for fixed chunking
          example: "\n\n"
          default: "\n\n"
        max_chunks:
          type: integer
          description: Maximum number of chunks to return
          example: 50
          default: 50
        threshold:
          type: number
          format: float
          description: Confidence threshold for ONNX models (0.0-1.0)
          example: 0.5
          default: 0.5

    ChunkRequest:
      type: object
      required:
        - text
      properties:
        text:
          type: string
          description: Text to chunk
          example: "This is a long document that needs to be split into smaller chunks..."
        config:
          $ref: "#/components/schemas/ChunkConfig"

    ChunkResponse:
      type: object
      required:
        - chunks
        - model
        - cache_hit
      example:
        {
          "chunks":
            [
              {
                "id": 0,
                "text": "This is the first chunk...",
                "start_char": 0,
                "end_char": 100,
              },
              {
                "id": 1,
                "text": "This is the second chunk...",
                "start_char": 90,
                "end_char": 190,
              },
            ],
          "model": "fixed",
          "cache_hit": false,
        }
      properties:
        chunks:
          type: array
          items:
            $ref: "#/components/schemas/Chunk"
          description: Array of text chunks
        model:
          type: string
          description: Chunking model actually used (may differ from requested if fallback occurred)
          example: "fixed"
        cache_hit:
          type: boolean
          description: Whether result was served from cache

    # Reranking Types
    RerankRequest:
      type: object
      required:
        - model
        - query
        - prompts
      properties:
        model:
          type: string
          description: Name of reranking model from models_dir/rerankers/
          example: "bge-reranker-v2-m3"
        query:
          type: string
          description: Search query for relevance scoring
          example: "machine learning applications"
        prompts:
          type: array
          items:
            type: string
          description: |
            Pre-rendered document texts to rerank. The client is responsible for extracting
            and rendering document fields/templates before calling this endpoint.
          example:
            [
              "Introduction to machine learning...",
              "Deep learning fundamentals...",
            ]

    RerankResponse:
      type: object
      required:
        - model
        - scores
      properties:
        model:
          type: string
          description: Name of model used for reranking
        scores:
          type: array
          items:
            type: number
            format: float
          description: Relevance scores (one per prompt, same order as input)

    # Recognizer Types
    RecognizeEntity:
      type: object
      required:
        - text
        - label
        - start
        - end
        - score
      properties:
        text:
          type: string
          description: The entity text
          example: "John Smith"
        label:
          type: string
          description: Entity type (PER, ORG, LOC, MISC)
          example: "PER"
        start:
          type: integer
          description: Character offset where entity begins
          example: 0
        end:
          type: integer
          description: Character offset where entity ends (exclusive)
          example: 10
        score:
          type: number
          format: float
          description: Confidence score (0.0 to 1.0)
          example: 0.99

    RecognizeRequest:
      type: object
      required:
        - model
        - texts
      properties:
        model:
          type: string
          description: Name of recognizer model from models_dir/recognizers/
          example: "bert-base-ner"
        texts:
          type: array
          items:
            type: string
          description: Texts to extract entities from
          example: ["John Smith works at Google.", "Apple Inc. is in Cupertino."]
        labels:
          type: array
          items:
            type: string
          description: |
            Custom entity labels to extract (GLiNER models only).
            When using a GLiNER model, you can specify any entity types to extract,
            enabling zero-shot NER without model retraining.
            If not provided, the model's default labels are used.
          example: ["person", "company", "product", "date"]
        relation_labels:
          type: array
          items:
            type: string
          description: |
            Relation types to extract (for models with 'relations' capability).
            Only used when the model supports relation extraction (GLiNER multitask, REBEL).
            If not provided, the model extracts all relations it can detect.
          example: ["founded", "works_at", "located_in"]

    Relation:
      type: object
      required:
        - head
        - tail
        - label
        - score
      properties:
        head:
          $ref: "#/components/schemas/RecognizeEntity"
          description: The subject/head entity in the relationship
        tail:
          $ref: "#/components/schemas/RecognizeEntity"
          description: The object/tail entity in the relationship
        label:
          type: string
          description: The relationship type
          example: "founded"
        score:
          type: number
          format: float
          description: Confidence score for the relation (0.0 to 1.0)
          example: 0.95

    RecognizeResponse:
      type: object
      required:
        - model
        - entities
      properties:
        model:
          type: string
          description: Name of model used for NER
        entities:
          type: array
          items:
            type: array
            items:
              $ref: "#/components/schemas/RecognizeEntity"
          description: Array of entity arrays (one per input text)
        relations:
          type: array
          items:
            type: array
            items:
              $ref: "#/components/schemas/Relation"
          description: |
            Array of relation arrays (one per input text).
            Only present when using a model with 'relations' capability (GLiNER multitask, REBEL).

    # Rewriter Types (Seq2Seq)
    RewriteRequest:
      type: object
      required:
        - model
        - inputs
      properties:
        model:
          type: string
          description: Name of Seq2Seq rewriter model from models_dir/rewriters/
          example: "flan-t5-small"
        inputs:
          type: array
          items:
            type: string
          description: Input texts to rewrite/transform
          example: ["Translate to German: Hello, how are you?"]

    RewriteResponse:
      type: object
      required:
        - model
        - texts
      properties:
        model:
          type: string
          description: Name of model used for rewriting
        texts:
          type: array
          items:
            type: array
            items:
              type: string
          description: Rewritten texts (array of arrays, one per input, multiple per beam)

    # Models Types
    ModelsResponse:
      type: object
      required:
        - chunkers
        - rerankers
        - embedders
        - generators
        - recognizers
        - rewriters
      properties:
        chunkers:
          type: array
          items:
            type: string
          description: Available chunking models (always includes "fixed")
          example: ["fixed", "chonky-mmbert-small-multilingual-1"]
        rerankers:
          type: array
          items:
            type: string
          description: Available reranking models
          example: ["bge-reranker-v2-m3"]
        embedders:
          type: array
          items:
            type: string
          description: Available embedding models from models_dir/embedders/
          example: ["bge-small-en-v1.5", "bge-small-en-v1.5-i8-qt"]
        generators:
          type: array
          items:
            type: string
          description: Available generator/LLM models from models_dir/generators/
          example: ["gemma-3-1b-it", "tiny-random-gemma-3"]
        recognizers:
          type: array
          items:
            type: string
          description: Available recognizer models from models_dir/recognizers/
          example: ["bert-base-ner", "distilbert-ner", "gliner-small"]
        extractors:
          type: array
          items:
            type: string
          description: Available GLiNER extractor models (zero-shot recognition with custom labels)
          example: ["gliner-small", "gliner-multitask"]
        rewriters:
          type: array
          items:
            type: string
          description: Available Seq2Seq rewriter models from models_dir/rewriters/
          example: ["flan-t5-small", "lmqg-question-generation"]
        recognizer_info:
          type: object
          additionalProperties:
            $ref: '#/components/schemas/RecognizerModelInfo'
          description: |
            Detailed information about recognizer models including capabilities.
            Map of model name to model info. Use this to determine what capabilities
            each recognizer supports (labels, zeroshot, relations, answers).
          example:
            bert-base-ner:
              capabilities: ["labels"]
            gliner-small:
              capabilities: ["labels", "zeroshot"]
            gliner-multitask:
              capabilities: ["labels", "zeroshot", "relations", "answers"]

    RecognizerCapability:
      type: string
      enum:
        - labels
        - zeroshot
        - relations
        - answers
      description: |
        Capability that a recognizer model supports:
        - labels: Entity extraction (NER) - extracts labeled spans like PER, ORG, LOC
        - zeroshot: Supports arbitrary labels at inference time (GLiNER models)
        - relations: Relation extraction between entities (GLiNER multitask, REBEL)
        - answers: Extractive question answering (GLiNER multitask)

    RecognizerModelInfo:
      type: object
      description: Detailed information about a recognizer model
      properties:
        capabilities:
          type: array
          items:
            $ref: '#/components/schemas/RecognizerCapability'
          description: List of capabilities this recognizer model supports
          example: ["labels", "zeroshot"]

    # Generation Types (LLM/Chat)
    Role:
      type: string
      enum: [system, user, assistant]
      description: The role of a message sender in a conversation

    FinishReason:
      type: string
      enum:
        - stop
        - length
        - tool_calls
        - content_filter
        - function_call
      description: Reason why generation stopped

    ChatMessage:
      type: object
      required:
        - role
        - content
      properties:
        role:
          $ref: "#/components/schemas/Role"
        content:
          type: string
          description: The message content

    GenerateRequest:
      type: object
      required:
        - model
        - messages
      properties:
        model:
          type: string
          description: Name of the generator model from models_dir/generators/
          example: "gemma-3-1b-it"
        messages:
          type: array
          items:
            $ref: "#/components/schemas/ChatMessage"
          description: Conversation messages (OpenAI-compatible format)
        max_tokens:
          type: integer
          description: Maximum tokens to generate
          default: 256
          example: 256
        temperature:
          type: number
          format: float
          description: Sampling temperature (0.0 = deterministic, higher = more random)
          default: 1.0
          example: 0.7
        top_p:
          type: number
          format: float
          description: Nucleus sampling probability
          default: 1.0
        top_k:
          type: integer
          description: Top-k sampling (Termite extension, not in OpenAI API)
          default: 50
        stream:
          type: boolean
          description: If true, partial message deltas will be sent as SSE events
          default: false

    GenerateResponse:
      type: object
      description: OpenAI-compatible chat completion response
      required:
        - id
        - object
        - created
        - model
        - choices
        - usage
      properties:
        id:
          type: string
          description: A unique identifier for the chat completion
          example: "chatcmpl-abc123"
        object:
          type: string
          enum: ["chat.completion"]
          description: The object type, always "chat.completion"
        created:
          type: integer
          description: Unix timestamp (seconds) when the completion was created
          example: 1704123456
        model:
          type: string
          description: Model used for generation
        choices:
          type: array
          items:
            $ref: "#/components/schemas/GenerateChoice"
          description: List of completion choices (currently always 1)
        usage:
          $ref: "#/components/schemas/GenerateUsage"

    GenerateChoice:
      type: object
      required:
        - index
        - message
        - finish_reason
      properties:
        index:
          type: integer
          description: Index of this choice in the list
        message:
          $ref: "#/components/schemas/GenerateMessage"
        finish_reason:
          $ref: "#/components/schemas/FinishReason"
        logprobs:
          type: object
          nullable: true
          description: Log probability information (not supported, always null)

    GenerateMessage:
      type: object
      required:
        - role
        - content
      properties:
        role:
          $ref: "#/components/schemas/Role"
        content:
          type: string
          description: The generated message content

    GenerateUsage:
      type: object
      required:
        - prompt_tokens
        - completion_tokens
        - total_tokens
      properties:
        prompt_tokens:
          type: integer
          description: Number of tokens in the prompt
        completion_tokens:
          type: integer
          description: Number of tokens in the completion
        total_tokens:
          type: integer
          description: Total tokens used (prompt + completion)

    GenerateChunk:
      type: object
      description: Streaming generation chunk (SSE event data)
      required:
        - id
        - object
        - created
        - model
        - choices
      properties:
        id:
          type: string
        object:
          type: string
          enum: ["chat.completion.chunk"]
        created:
          type: integer
        model:
          type: string
        choices:
          type: array
          items:
            $ref: "#/components/schemas/GenerateChunkChoice"

    GenerateChunkChoice:
      type: object
      required:
        - index
        - delta
      properties:
        index:
          type: integer
        delta:
          $ref: "#/components/schemas/GenerateDelta"
        finish_reason:
          $ref: "#/components/schemas/FinishReason"

    GenerateDelta:
      type: object
      description: Delta content for streaming
      properties:
        role:
          $ref: "#/components/schemas/Role"
        content:
          type: string
          nullable: true
          description: Token content delta

    Config:
      type: object
      required:
        - api_url
      properties:
        api_url:
          type: string
          format: uri
          description: "URL of the Termite embedding/chunking service"
          example: "http://localhost:8080"
        models_dir:
          type: string
          description: |
            Base directory containing model subdirectories. Termite auto-discovers models from:
            - `{models_dir}/embedders/` - Embedding models (ONNX)
            - `{models_dir}/chunkers/` - Chunking models (ONNX)
            - `{models_dir}/rerankers/` - Reranking models (ONNX)
            - `{models_dir}/recognizers/` - Recognition models (ONNX)
            - `{models_dir}/rewriters/` - Seq2Seq rewriter models (ONNX)

            Defaults to ~/.termite/models (set via viper). If not set, only built-in fixed chunking is available.
          example: "~/.termite/models"
        content_security:
          $ref: "../../../antfly-go/libaf/scraping/openapi.yaml#/components/schemas/ContentSecurityConfig"
          description: "Security settings for downloading content from URLs (e.g., images for CLIP models). Controls allowed hosts, private IP blocking, download limits, and timeouts."
        s3_credentials:
          $ref: "../../../antfly-go/libaf/s3/openapi.yaml#/components/schemas/Credentials"
          description: "S3 credentials for downloading content from S3 URLs. If not set, S3 URLs will fail."
        keep_alive:
          type: string
          description: |
            How long to keep models loaded in memory after last use (Ollama-compatible).
            Models are automatically unloaded after this duration of inactivity.
            Use Go duration format: "5m" (5 minutes), "1h" (1 hour), "0" (never unload, eager loading).
            When set to "0" or omitted, models are loaded eagerly at startup and never unloaded (legacy behavior).
          default: "0"
          example: "5m"
        max_loaded_models:
          type: integer
          description: |
            Maximum number of models to keep loaded in memory simultaneously.
            When this limit is reached, the least recently used model is unloaded (LRU eviction).
            Set to 0 for unlimited (default). Only effective when keep_alive is non-zero.
          default: 0
          example: 3
        backend_priority:
          type: array
          items:
            type: string
          description: |
            Backend priority order for model loading with optional device specifiers.
            Format: `backend` or `backend:device` where device defaults to `auto`.

            Termite tries entries in order and uses the first available backend+device
            combination that supports the model.

            **Backends** (depend on build tags):
            - `go` - Pure Go inference (always available, CPU only, slowest)
            - `onnx` - ONNX Runtime (requires -tags="onnx,ORT", fastest)
            - `xla` - GoMLX XLA (requires -tags="xla,XLA", TPU/CUDA/CPU)

            **Devices**:
            - `auto` - Auto-detect best available (default)
            - `cuda` - NVIDIA CUDA GPU
            - `coreml` - Apple CoreML (macOS only, used by ONNX)
            - `tpu` - Google TPU (used by XLA)
            - `cpu` - Force CPU only

            **Examples**:
            - `["onnx", "xla", "go"]` - Try backends with auto device detection
            - `["onnx:cuda", "xla:tpu", "onnx:cpu", "go"]` - Prefer GPU, fall back to CPU
            - `["onnx:coreml", "go"]` - macOS with CoreML acceleration
          default: [onnx, xla, go]
          example: ["onnx:cuda", "xla:tpu", "onnx:cpu", "xla:cpu", "go"]
        max_concurrent_requests:
          type: integer
          description: |
            Maximum number of concurrent inference requests allowed.
            Additional requests will be queued up to max_queue_size.
            Set to 0 for unlimited (default).
          default: 0
          example: 4
        max_queue_size:
          type: integer
          description: |
            Maximum number of requests to queue when max_concurrent_requests is reached.
            When the queue is full, new requests receive 503 Service Unavailable with Retry-After header.
            Set to 0 for unlimited queue (default). Only effective when max_concurrent_requests > 0.
          default: 0
          example: 100
        request_timeout:
          type: string
          description: |
            Maximum time to wait for a request to complete, including queue wait time.
            Use Go duration format: "30s", "1m", "0" (no timeout, default).
            Requests exceeding this timeout receive 504 Gateway Timeout.
          default: "0"
          example: "30s"
        preload:
          type: array
          items:
            type: string
          description: |
            List of model names to preload at startup (Ollama-compatible).
            These models are loaded immediately when Termite starts, avoiding first-request latency.
            Model names should match those in models_dir/embedders/ (e.g., "bge-small-en-v1.5").
            Only effective when keep_alive is non-zero (lazy loading mode).
          example: ["bge-small-en-v1.5", "clip-vit-base-patch32"]
        max_memory_mb:
          type: integer
          description: |
            Maximum memory (in MB) to use for loaded models.
            When this limit is approached, least recently used models are unloaded.
            Set to 0 for unlimited (default). This is an advisory limit - actual memory
            usage depends on model sizes and may temporarily exceed this value.
            Works alongside max_loaded_models for fine-grained control.
          default: 0
          example: 4096
        model_strategies:
          type: object
          additionalProperties:
            type: string
            enum: [eager, lazy, bounded]
          description: |
            Per-model loading strategy overrides. Maps model names to their loading strategy.
            Models not in this map use the default strategy based on keep_alive:
            - If keep_alive="0" (default): eager loading (load at startup, never unload)
            - If keep_alive>0: lazy loading (load on demand, unload after idle)

            When a model has strategy "eager" in this map:
            - It is loaded at startup (as part of preload)
            - It is never unloaded, even when keep_alive>0 (pinned in memory)

            This allows mixing eager and lazy models in the same pool.
          example:
            bge-small-en-v1.5: eager
            chonky: lazy
        log:
          $ref: "../../../antfly-go/libaf/logging/openapi.yaml#/components/schemas/Config"

    VersionResponse:
      type: object
      required:
        - version
        - git_commit
        - build_time
        - go_version
      properties:
        version:
          type: string
          description: Termite version
          example: "v1.0.0"
        git_commit:
          type: string
          description: Git commit hash
          example: "abc1234"
        build_time:
          type: string
          description: Build timestamp
          example: "2024-01-15T10:30:00Z"
        go_version:
          type: string
          description: Go runtime version
          example: "go1.25.0"

paths:
  /embed:
    post:
      summary: Generate embeddings
      description: |
        Generates vector embeddings for input content using local ONNX models.
        This endpoint is compatible with Ollama's `/api/embed` format for text,
        and extends it with OpenAI-compatible multimodal support for CLIP models.

        ## Models

        Models are auto-discovered from `models_dir/embedders/` at startup.
        Use the `/api/models` endpoint to list available models.

        - **Text-only models** (e.g., bge-small-en-v1.5): Accept text strings
        - **Multimodal models** (e.g., CLIP): Accept text and images via data URIs

        ## Input Formats

        Three formats are supported:
        - Single text string: `"hello world"`
        - Array of text strings: `["hello", "world"]` (Ollama-compatible)
        - Array of content parts: `[{"type": "text", "text": "..."}, {"type": "image_url", "image_url": {"url": "data:image/png;base64,..."}}]` (OpenAI-compatible)

        ## Caching

        Results are cached in memory for 2 minutes. Concurrent identical requests are deduplicated
        using singleflight to prevent redundant work.

        ## Response Formats

        Supports multiple content types via Accept header:
        - `application/octet-stream`: Binary serialization (default, most efficient)
        - `application/json`: JSON response with model name and embeddings

        ## Examples

        Text embedding (Ollama-compatible):
        ```json
        {
          "model": "bge-small-en-v1.5",
          "input": ["hello world", "machine learning"]
        }
        ```

        Multimodal embedding (OpenAI-compatible):
        ```json
        {
          "model": "clip-vit-base-patch32",
          "input": [
            {"type": "text", "text": "a photo of a cat"},
            {"type": "image_url", "image_url": {"url": "data:image/png;base64,iVBORw0..."}}
          ]
        }
        ```
      operationId: generateEmbeddings
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/EmbedRequest"
      responses:
        "200":
          description: Embeddings generated successfully
          content:
            application/octet-stream:
              schema:
                type: string
                format: binary
                description: Binary-serialized embedding vectors
            application/json:
              schema:
                $ref: "#/components/schemas/EmbedResponse"
        "400":
          description: Invalid request
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Error"
        "404":
          description: Model not found
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Error"
        "500":
          description: Internal server error
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Error"

  /chunk:
    post:
      summary: Chunk text into smaller segments
      description: |
        Splits text into smaller chunks using semantic or fixed-size chunking models.

        ## Models

        ### Fixed Chunking (always available)
        - Simple token-based splitting with overlap
        - Use model="fixed"
        - Fast and deterministic

        ### ONNX Models
        - Semantic chunking based on content similarity
        - Models auto-discovered from `models_dir/chunkers/`
        - Falls back to fixed chunking if model fails

        ## Caching

        Results are cached in memory for 2 minutes. Cache key includes both config and text content.

        ## Example

        ```json
        {
          "text": "This is a long document...",
          "config": {
            "model": "fixed",
            "target_tokens": 500,
            "overlap_tokens": 50,
            "separator": "\n\n"
          }
        }
        ```
      operationId: chunkText
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/ChunkRequest"
      responses:
        "200":
          description: Text chunked successfully
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ChunkResponse"
        "400":
          description: Invalid request
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Error"
        "500":
          description: Internal server error
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Error"

  /rerank:
    post:
      summary: Rerank prompts by relevance
      description: |
        Re-scores pre-rendered text prompts based on relevance to a query using ONNX reranking models.

        ## Client Responsibilities

        The client must:
        1. Extract relevant fields from documents
        2. Render any templates
        3. Send pre-rendered text strings as `prompts`

        This design keeps Termite stateless and allows clients to customize rendering logic.

        ## Models

        - Models are auto-discovered from `models_dir/rerankers/`
        - Supports quantized models (`model_quantized.onnx`)
        - Automatically prefers quantized variants if available

        ## Example

        ```json
        {
          "model": "bge-reranker-v2-m3",
          "query": "machine learning applications",
          "prompts": [
            "Introduction to Machine Learning: This guide covers...",
            "Deep Learning Fundamentals: Neural networks are..."
          ]
        }
        ```

        For document-based reranking with field extraction, use the client-side
        `lib/reranking` package which handles rendering before calling this endpoint.
      operationId: rerankPrompts
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/RerankRequest"
      responses:
        "200":
          description: Prompts reranked successfully
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/RerankResponse"
        "400":
          description: Invalid request
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Error"
        "404":
          description: Model not found
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Error"
        "503":
          description: Reranking service unavailable (no models configured)
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Error"
        "500":
          description: Internal server error
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Error"

  /generate:
    post:
      summary: Generate text using LLM (OpenAI-compatible)
      description: |
        Generates text using local LLM models (e.g., Gemma 3).
        Fully compatible with the OpenAI Chat Completions API.

        ## Models

        Models are auto-discovered from `models_dir/generators/` at startup.
        Use the `/api/models` endpoint to list available models.

        ## Streaming

        Set `stream: true` to receive Server-Sent Events (SSE) with incremental
        token deltas. Each event contains a `ChatCompletionChunk` object.
        The stream ends with `data: [DONE]`.

        ## Input Format

        Uses OpenAI-compatible chat format with messages array:
        ```json
        {
          "model": "gemma-3-1b-it",
          "messages": [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Hello!"}
          ],
          "max_tokens": 256,
          "stream": false
        }
        ```

        ## Example (Non-streaming)

        ```bash
        curl -X POST http://localhost:8080/api/generate \
          -H "Content-Type: application/json" \
          -d '{
            "model": "gemma-3-1b-it",
            "messages": [{"role": "user", "content": "What is machine learning?"}],
            "max_tokens": 100
          }'
        ```

        ## Example (Streaming)

        ```bash
        curl -X POST http://localhost:8080/api/generate \
          -H "Content-Type: application/json" \
          -d '{
            "model": "gemma-3-1b-it",
            "messages": [{"role": "user", "content": "Hello!"}],
            "stream": true
          }'
        ```
      operationId: generateContent
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/GenerateRequest"
      responses:
        "200":
          description: |
            Chat completion response. Returns JSON for non-streaming requests,
            or Server-Sent Events for streaming requests (stream: true).
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/GenerateResponse"
            text/event-stream:
              schema:
                $ref: "#/components/schemas/GenerateChunk"
        "400":
          description: Invalid request
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Error"
        "404":
          description: Model not found
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Error"
        "503":
          description: Generation service unavailable (no models configured)
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Error"
        "500":
          description: Internal server error
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Error"

  /recognize:
    post:
      summary: Recognize named entities
      description: |
        Recognizes named entities (persons, organizations, locations, etc.) from text using ONNX recognition models.

        ## Entity Types

        Standard CoNLL entity types:
        - **PER**: Person names (e.g., "John Smith")
        - **ORG**: Organizations (e.g., "Google", "Apple Inc.")
        - **LOC**: Locations (e.g., "New York", "France")
        - **MISC**: Miscellaneous entities

        ## Models

        - Models are auto-discovered from `models_dir/recognizers/`
        - Supports quantized variants (model_i8.onnx)
        - Compatible with HuggingFace BERT-based recognition models
        - GLiNER models support custom entity labels via the `labels` parameter

        ## Example

        ```json
        {
          "model": "bert-base-ner",
          "texts": ["John Smith works at Google.", "Apple Inc. is in Cupertino."]
        }
        ```
      operationId: recognizeEntities
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/RecognizeRequest"
      responses:
        "200":
          description: Entities extracted successfully
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/RecognizeResponse"
        "400":
          description: Invalid request
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Error"
        "404":
          description: Model not found
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Error"
        "503":
          description: Recognition service unavailable (no models configured)
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Error"
        "500":
          description: Internal server error
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Error"

  /rewrite:
    post:
      summary: Rewrite text using Seq2Seq models
      description: |
        Rewrite/transform text using Seq2Seq models (T5, FLAN-T5, BART, etc.).

        ## Models

        - Models are auto-discovered from `models_dir/rewriters/`
        - Seq2Seq models have encoder.onnx, decoder-init.onnx, and decoder.onnx files
        - Compatible with LMQG question generation models

        ## Use Cases

        - **Question Generation**: Generate questions from answer-context pairs
        - **Query Generation**: Generate search queries from documents
        - **Paraphrasing**: Rewrite text in different words
        - **Translation**: Translate text between languages

        ## Example

        For question generation with LMQG models:
        ```json
        {
          "model": "flan-t5-small-squad-qg",
          "inputs": ["generate question: <hl> Beyonce <hl> Beyonce starred as Etta James in Cadillac Records."]
        }
        ```
      operationId: rewriteText
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/RewriteRequest"
      responses:
        "200":
          description: Text rewritten successfully
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/RewriteResponse"
        "400":
          description: Invalid request
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Error"
        "404":
          description: Model not found
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Error"
        "503":
          description: Generation service unavailable (no models configured)
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Error"
        "500":
          description: Internal server error
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Error"

  /models:
    get:
      summary: List available models
      description: |
        Returns lists of available embedding, chunking, reranking, generator, NER, and rewriter models.

        ## Embedders

        - ONNX models from `models_dir/embedders/`
        - Quantized variants have `-i8` suffix

        ## Chunkers

        - Always includes "fixed" (built-in)
        - Plus any ONNX models from `models_dir/chunkers/`

        ## Rerankers

        - ONNX models from `models_dir/rerankers/`
        - Empty if no models configured

        ## Generators

        - LLM models from `models_dir/generators/`
        - Empty if no models configured

        ## Recognizers

        - ONNX models from `models_dir/recognizers/`
        - Includes GLiNER models for zero-shot recognition

        ## Rewriters

        - Seq2Seq models from `models_dir/rewriters/`
        - T5, FLAN-T5, BART, and LMQG question generation models

        Models are discovered at service startup and cached.
      operationId: listModels
      responses:
        "200":
          description: Models retrieved successfully
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ModelsResponse"
        "400":
          description: Bad request
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Error"
        "500":
          description: Internal server error
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Error"

  /version:
    get:
      summary: Get version information
      description: Returns Termite version, git commit, build time, and Go runtime version.
      operationId: getVersion
      responses:
        "200":
          description: Version information
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/VersionResponse"
        "400":
          description: Bad request
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Error"
