openapi: 3.0.3
info:
  version: v1alpha1
  title: Termite API
  description: |
    Termite is an Ollama-like local inference server for ONNX-based ML models.

    ## What is Termite

    Termite provides local ML inference with an Ollama-compatible API:
    - **Embedding Generation**: Text and multimodal (CLIP) embedding models
    - **Text Chunking**: Semantic chunking with ONNX models or fixed-size fallback
    - **Reranking**: Relevance re-scoring for search results
    - **Named Entity Recognition (NER)**: Extract persons, organizations, locations from text
    - **Future**: Classification and generative model support planned

    Download the latest release at https://antfly.io/docs/downloads

    ## When to Use Termite

    Termite can run standalone or as part of an Antfly cluster:
    - Local ONNX model inference without external API dependencies
    - Ollama-compatible `/api/embed` endpoint for embeddings
    - Semantic text chunking for RAG pipelines
    - Relevance reranking for improved search quality
    - Centralized model serving across distributed nodes
    - Privacy-preserving ML inference (data never leaves your infrastructure)

    ## Features

    ### Embedding Generation
    - **Models**: ONNX models auto-discovered from `{models_dir}/embedders/`
    - **API**: Ollama-compatible `/api/embed` endpoint
    - **Response Formats**: Binary (default), JSON

    ### Multimodal Support (CLIP)
    - **Image Embeddings**: CLIP models for joint text-image embedding space
    - **Input Formats**: Base64 data URIs (`data:image/png;base64,...`) or URLs
    - **OpenAI-Compatible**: Uses content parts format (`{"type": "image_url", "image_url": {"url": "..."}}`)
    - **Use Cases**: Image search, cross-modal retrieval, visual similarity

    ### Text Chunking
    - **Models**: Fixed-size chunking (always available) + ONNX models
    - **Model Discovery**: Auto-discovers models from `{models_dir}/chunkers/`
    - **Caching**: 2-minute TTL memory cache
    - **Fallback**: Falls back to fixed chunking if model fails

    ### Reranking
    - **Model Discovery**: Auto-discovers ONNX models from `{models_dir}/rerankers/`
    - **Quantization**: Automatically uses quantized models if available
    - **Input**: Pre-rendered text prompts (client handles field extraction)
  contact:
    name: Antfly
    url: https://github.com/antflydb/termite
  license:
    name: All Rights Reserved
    url: https://antfly.io/legal
servers:
  - url: /api
    description: Termite API server
security: []
paths:
  /embed:
    post:
      summary: Generate embeddings
      description: |
        Generates vector embeddings for input content using local ONNX models.
        This endpoint is compatible with Ollama's `/api/embed` format for text,
        and extends it with OpenAI-compatible multimodal support for CLIP models.

        ## Models

        Models are auto-discovered from `models_dir/embedders/` at startup.
        Use the `/api/models` endpoint to list available models.

        - **Text-only models** (e.g., bge-small-en-v1.5): Accept text strings
        - **Multimodal models** (e.g., CLIP): Accept text and images via data URIs

        ## Input Formats

        Three formats are supported:
        - Single text string: `"hello world"`
        - Array of text strings: `["hello", "world"]` (Ollama-compatible)
        - Array of content parts: `[{"type": "text", "text": "..."}, {"type": "image_url", "image_url": {"url": "data:image/png;base64,..."}}]` (OpenAI-compatible)

        ## Caching

        Results are cached in memory for 2 minutes. Concurrent identical requests are deduplicated
        using singleflight to prevent redundant work.

        ## Response Formats

        Supports multiple content types via Accept header:
        - `application/octet-stream`: Binary serialization (default, most efficient)
        - `application/json`: JSON response with model name and embeddings

        ## Examples

        Text embedding (Ollama-compatible):
        ```json
        {
          "model": "bge-small-en-v1.5",
          "input": ["hello world", "machine learning"]
        }
        ```

        Multimodal embedding (OpenAI-compatible):
        ```json
        {
          "model": "clip-vit-base-patch32",
          "input": [
            {"type": "text", "text": "a photo of a cat"},
            {"type": "image_url", "image_url": {"url": "data:image/png;base64,iVBORw0..."}}
          ]
        }
        ```
      operationId: generateEmbeddings
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/EmbedRequest'
      responses:
        '200':
          description: Embeddings generated successfully
          content:
            application/octet-stream:
              schema:
                type: string
                format: binary
                description: Binary-serialized embedding vectors
            application/json:
              schema:
                $ref: '#/components/schemas/EmbedResponse'
        '400':
          description: Invalid request
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'
        '404':
          description: Model not found
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'
        '500':
          description: Internal server error
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'
  /chunk:
    post:
      summary: Chunk text into smaller segments
      description: |
        Splits text into smaller chunks using semantic or fixed-size chunking models.

        ## Models

        ### Fixed Chunking (always available)
        - Simple token-based splitting with overlap
        - Use model="fixed"
        - Fast and deterministic

        ### ONNX Models
        - Semantic chunking based on content similarity
        - Models auto-discovered from `models_dir/chunkers/`
        - Falls back to fixed chunking if model fails

        ## Caching

        Results are cached in memory for 2 minutes. Cache key includes both config and text content.

        ## Example

        ```json
        {
          "text": "This is a long document...",
          "config": {
            "model": "fixed",
            "target_tokens": 500,
            "overlap_tokens": 50,
            "separator": "\n\n"
          }
        }
        ```
      operationId: chunkText
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/ChunkRequest'
      responses:
        '200':
          description: Text chunked successfully
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ChunkResponse'
        '400':
          description: Invalid request
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'
        '500':
          description: Internal server error
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'
  /rerank:
    post:
      summary: Rerank prompts by relevance
      description: |
        Re-scores pre-rendered text prompts based on relevance to a query using ONNX reranking models.

        ## Client Responsibilities

        The client must:
        1. Extract relevant fields from documents
        2. Render any templates
        3. Send pre-rendered text strings as `prompts`

        This design keeps Termite stateless and allows clients to customize rendering logic.

        ## Models

        - Models are auto-discovered from `models_dir/rerankers/`
        - Supports quantized models (`model_quantized.onnx`)
        - Automatically prefers quantized variants if available

        ## Example

        ```json
        {
          "model": "bge-reranker-v2-m3",
          "query": "machine learning applications",
          "prompts": [
            "Introduction to Machine Learning: This guide covers...",
            "Deep Learning Fundamentals: Neural networks are..."
          ]
        }
        ```

        For document-based reranking with field extraction, use the client-side
        `lib/reranking` package which handles rendering before calling this endpoint.
      operationId: rerankPrompts
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/RerankRequest'
      responses:
        '200':
          description: Prompts reranked successfully
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/RerankResponse'
        '400':
          description: Invalid request
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'
        '404':
          description: Model not found
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'
        '500':
          description: Internal server error
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'
        '503':
          description: Reranking service unavailable (no models configured)
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'
  /recognize:
    post:
      summary: Recognize named entities
      description: |
        Recognizes named entities (persons, organizations, locations, etc.) from text using ONNX NER models.

        ## Entity Types

        Standard CoNLL entity types:
        - **PER**: Person names (e.g., "John Smith")
        - **ORG**: Organizations (e.g., "Google", "Apple Inc.")
        - **LOC**: Locations (e.g., "New York", "France")
        - **MISC**: Miscellaneous entities

        ## Models

        - Models are auto-discovered from `models_dir/ner/`
        - Supports quantized variants (model_i8.onnx)
        - Compatible with HuggingFace BERT-based NER models
        - GLiNER models support custom entity labels via the `labels` parameter

        ## Example

        ```json
        {
          "model": "bert-base-ner",
          "texts": ["John Smith works at Google.", "Apple Inc. is in Cupertino."]
        }
        ```
      operationId: recognizeEntities
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/NERRequest'
      responses:
        '200':
          description: Entities extracted successfully
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/NERResponse'
        '400':
          description: Invalid request
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'
        '404':
          description: Model not found
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'
        '500':
          description: Internal server error
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'
        '503':
          description: NER service unavailable (no models configured)
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'
  /question:
    post:
      summary: Generate questions
      description: |
        Generate questions using Seq2Seq models (T5, FLAN-T5, BART, etc.).

        ## Models

        - Models are auto-discovered from `models_dir/generators/`
        - Seq2Seq models have encoder.onnx, decoder-init.onnx, and decoder.onnx files
        - Compatible with LMQG question generation models

        ## Use Cases

        - **Question Generation**: Generate questions from answer-context pairs
        - **Query Generation**: Generate search queries from documents

        ## Example

        For question generation with LMQG models:
        ```json
        {
          "model": "flan-t5-small-squad-qg",
          "inputs": ["generate question: <hl> Beyonce <hl> Beyonce starred as Etta James in Cadillac Records."]
        }
        ```
      operationId: generateQuestions
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/GenerateRequest'
      responses:
        '200':
          description: Text generated successfully
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/GenerateResponse'
        '400':
          description: Invalid request
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'
        '404':
          description: Model not found
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'
        '500':
          description: Internal server error
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'
        '503':
          description: Generation service unavailable (no models configured)
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'
  /models:
    get:
      summary: List available models
      description: |
        Returns lists of available embedding, chunking, reranking, NER, and generation models.

        ## Embedders

        - ONNX models from `models_dir/embedders/`
        - Quantized variants have `-i8` suffix

        ## Chunkers

        - Always includes "fixed" (built-in)
        - Plus any ONNX models from `models_dir/chunkers/`

        ## Rerankers

        - ONNX models from `models_dir/rerankers/`
        - Empty if no models configured

        ## NER

        - ONNX models from `models_dir/ner/`
        - Includes GLiNER models for zero-shot NER

        ## Generators

        - Seq2Seq models from `models_dir/generators/`
        - T5, FLAN-T5, BART, and LMQG question generation models

        Models are discovered at service startup and cached.
      operationId: listModels
      responses:
        '200':
          description: Models retrieved successfully
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ModelsResponse'
        '400':
          description: Bad request
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'
        '500':
          description: Internal server error
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'
  /version:
    get:
      summary: Get version information
      description: Returns Termite version, git commit, build time, and Go runtime version.
      operationId: getVersion
      responses:
        '200':
          description: Version information
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/VersionResponse'
        '400':
          description: Bad request
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'
components:
  schemas:
    Error:
      type: object
      required:
        - error
      properties:
        error:
          type: string
          description: Error message
    TextContentPart:
      type: object
      description: Text content for embedding
      required:
        - type
        - text
      properties:
        type:
          type: string
          enum:
            - text
        text:
          type: string
          description: Text content to embed
    ImageURL:
      type: object
      description: Image URL or data URI
      required:
        - url
      properties:
        url:
          type: string
          description: URL or data URI (data:image/png;base64,...)
          example: data:image/png;base64,iVBORw0KGgo...
    ImageURLContentPart:
      type: object
      description: Image content for embedding (OpenAI-compatible format)
      required:
        - type
        - image_url
      properties:
        type:
          type: string
          enum:
            - image_url
        image_url:
          $ref: '#/components/schemas/ImageURL'
    ContentPart:
      description: A content part for multimodal embedding (text or image)
      oneOf:
        - $ref: '#/components/schemas/TextContentPart'
        - $ref: '#/components/schemas/ImageURLContentPart'
    EmbedRequest:
      type: object
      required:
        - model
        - input
      properties:
        model:
          type: string
          description: Name of the embedder model from models_dir/embedders/
          example: bge-small-en-v1.5
        input:
          oneOf:
            - type: string
              description: Single text string (backward compatible)
            - type: array
              items:
                type: string
              description: Array of text strings (backward compatible)
            - type: array
              items:
                $ref: '#/components/schemas/ContentPart'
              description: Array of multimodal content parts (text or images)
          description: |
            Input content to embed. Supports three formats:
            - Single text string: `"hello world"`
            - Array of text strings: `["hello", "world"]`
            - Array of content parts (multimodal): `[{"type": "text", "text": "hello"}, {"type": "image_url", "image_url": {"url": "data:image/png;base64,..."}}]`
          example:
            - hello world
            - machine learning
        truncate:
          type: boolean
          default: true
          description: Truncate input to fit model context length
    EmbedResponse:
      type: object
      example:
        model: bge-small-en-v1.5
        embeddings:
          - - 0.0123
            - -0.0456
            - 0.0789
          - - 0.0234
            - -0.0567
            - 0.089
      required:
        - model
        - embeddings
      properties:
        model:
          type: string
          description: Model used for embedding
          example: bge-small-en-v1.5
        embeddings:
          type: array
          items:
            type: array
            items:
              type: number
              format: float
          description: Array of embedding vectors (one per input string)
    Chunk:
      type: object
      description: A chunk of text with position information.
      required:
        - id
        - text
        - start_char
        - end_char
      properties:
        id:
          type: integer
          description: Sequence number of the chunk (0, 1, 2, ...)
          x-go-type: uint32
        text:
          type: string
          description: The chunk content
        start_char:
          type: integer
          description: Character position in original text where chunk starts
        end_char:
          type: integer
          description: Character position in original text where chunk ends (exclusive)
    ChunkConfig:
      type: object
      description: |
        Configuration for chunking requests to Termite API.
        This is a simplified config for the HTTP API - differs from the full ChunkerConfig
        which includes provider selection and caching configuration.
      properties:
        model:
          type: string
          description: The chunking model to use. Either 'fixed' for simple token-based chunking, or a model name from models/chunkers/{name}/.
          default: fixed
          example: fixed
        target_tokens:
          type: integer
          description: Target number of tokens per chunk
          example: 500
          default: 500
        overlap_tokens:
          type: integer
          description: Number of overlapping tokens between chunks
          example: 50
          default: 50
        separator:
          type: string
          description: Text separator for fixed chunking
          example: |+


          default: |+


        max_chunks:
          type: integer
          description: Maximum number of chunks to return
          example: 50
          default: 50
        threshold:
          type: number
          format: float
          description: Confidence threshold for ONNX models (0.0-1.0)
          example: 0.5
          default: 0.5
    ChunkRequest:
      type: object
      required:
        - text
      properties:
        text:
          type: string
          description: Text to chunk
          example: This is a long document that needs to be split into smaller chunks...
        config:
          $ref: '#/components/schemas/ChunkConfig'
    ChunkResponse:
      type: object
      required:
        - chunks
        - model
        - cache_hit
      example:
        chunks:
          - id: 0
            text: This is the first chunk...
            start_char: 0
            end_char: 100
          - id: 1
            text: This is the second chunk...
            start_char: 90
            end_char: 190
        model: fixed
        cache_hit: false
      properties:
        chunks:
          type: array
          items:
            $ref: '#/components/schemas/Chunk'
          description: Array of text chunks
        model:
          type: string
          description: Chunking model actually used (may differ from requested if fallback occurred)
          example: fixed
        cache_hit:
          type: boolean
          description: Whether result was served from cache
    RerankRequest:
      type: object
      required:
        - model
        - query
        - prompts
      properties:
        model:
          type: string
          description: Name of reranking model from models_dir/rerankers/
          example: bge-reranker-v2-m3
        query:
          type: string
          description: Search query for relevance scoring
          example: machine learning applications
        prompts:
          type: array
          items:
            type: string
          description: |
            Pre-rendered document texts to rerank. The client is responsible for extracting
            and rendering document fields/templates before calling this endpoint.
          example:
            - Introduction to machine learning...
            - Deep learning fundamentals...
    RerankResponse:
      type: object
      required:
        - model
        - scores
      properties:
        model:
          type: string
          description: Name of model used for reranking
        scores:
          type: array
          items:
            type: number
            format: float
          description: Relevance scores (one per prompt, same order as input)
    NEREntity:
      type: object
      required:
        - text
        - label
        - start
        - end
        - score
      properties:
        text:
          type: string
          description: The entity text
          example: John Smith
        label:
          type: string
          description: Entity type (PER, ORG, LOC, MISC)
          example: PER
        start:
          type: integer
          description: Character offset where entity begins
          example: 0
        end:
          type: integer
          description: Character offset where entity ends (exclusive)
          example: 10
        score:
          type: number
          format: float
          description: Confidence score (0.0 to 1.0)
          example: 0.99
    NERRequest:
      type: object
      required:
        - model
        - texts
      properties:
        model:
          type: string
          description: Name of NER model from models_dir/ner/
          example: bert-base-ner
        texts:
          type: array
          items:
            type: string
          description: Texts to extract entities from
          example:
            - John Smith works at Google.
            - Apple Inc. is in Cupertino.
        labels:
          type: array
          items:
            type: string
          description: |
            Custom entity labels to extract (GLiNER models only).
            When using a GLiNER model, you can specify any entity types to extract,
            enabling zero-shot NER without model retraining.
            If not provided, the model's default labels are used.
          example:
            - person
            - company
            - product
            - date
    NERResponse:
      type: object
      required:
        - model
        - entities
      properties:
        model:
          type: string
          description: Name of model used for NER
        entities:
          type: array
          items:
            type: array
            items:
              $ref: '#/components/schemas/NEREntity'
          description: Array of entity arrays (one per input text)
    GenerateRequest:
      type: object
      required:
        - model
        - inputs
      properties:
        model:
          type: string
          description: Name of Seq2Seq model from models_dir/generators/
          example: flan-t5-small
        inputs:
          type: array
          items:
            type: string
          description: Input texts to generate from
          example:
            - 'Translate to German: Hello, how are you?'
    GenerateResponse:
      type: object
      required:
        - model
        - texts
      properties:
        model:
          type: string
          description: Name of model used for generation
        texts:
          type: array
          items:
            type: array
            items:
              type: string
          description: Generated texts (array of arrays, one per input, multiple per beam)
    ModelsResponse:
      type: object
      required:
        - chunkers
        - rerankers
        - embedders
        - ner
        - generators
      properties:
        chunkers:
          type: array
          items:
            type: string
          description: Available chunking models (always includes "fixed")
          example:
            - fixed
            - chonky-mmbert-small-multilingual-1
        rerankers:
          type: array
          items:
            type: string
          description: Available reranking models
          example:
            - bge-reranker-v2-m3
        embedders:
          type: array
          items:
            type: string
          description: Available embedding models from models_dir/embedders/
          example:
            - bge-small-en-v1.5
            - bge-small-en-v1.5-i8-qt
        ner:
          type: array
          items:
            type: string
          description: Available NER models from models_dir/ner/
          example:
            - bert-base-ner
            - distilbert-ner
            - gliner-small
        gliner:
          type: array
          items:
            type: string
          description: Available GLiNER models (zero-shot NER with custom labels)
          example:
            - gliner-small
            - gliner-multitask
        generators:
          type: array
          items:
            type: string
          description: Available Seq2Seq generation models from models_dir/generators/
          example:
            - flan-t5-small
            - lmqg-question-generation
    Config:
      type: object
      required:
        - api_url
      properties:
        api_url:
          type: string
          format: uri
          description: URL of the Termite embedding/chunking service
          example: http://localhost:8080
        models_dir:
          type: string
          description: |
            Base directory containing model subdirectories. Termite auto-discovers models from:
            - `{models_dir}/embedders/` - Embedding models (ONNX)
            - `{models_dir}/chunkers/` - Chunking models (ONNX)
            - `{models_dir}/rerankers/` - Reranking models (ONNX)

            Defaults to ~/.termite/models (set via viper). If not set, only built-in fixed chunking is available.
          example: ~/.termite/models
        content_security:
          $ref: '#/components/schemas/ContentSecurityConfig'
          description: Security settings for downloading content from URLs (e.g., images for CLIP models). Controls allowed hosts, private IP blocking, download limits, and timeouts.
        s3_credentials:
          $ref: '#/components/schemas/Credentials'
          description: S3 credentials for downloading content from S3 URLs. If not set, S3 URLs will fail.
        keep_alive:
          type: string
          description: |
            How long to keep models loaded in memory after last use (Ollama-compatible).
            Models are automatically unloaded after this duration of inactivity.
            Use Go duration format: "5m" (5 minutes), "1h" (1 hour), "0" (never unload, eager loading).
            When set to "0" or omitted, models are loaded eagerly at startup and never unloaded (legacy behavior).
          default: '0'
          example: 5m
        max_loaded_models:
          type: integer
          description: |
            Maximum number of models to keep loaded in memory simultaneously.
            When this limit is reached, the least recently used model is unloaded (LRU eviction).
            Set to 0 for unlimited (default). Only effective when keep_alive is non-zero.
          default: 0
          example: 3
        backend_priority:
          type: array
          items:
            type: string
          description: |
            Backend priority order for model loading with optional device specifiers.
            Format: `backend` or `backend:device` where device defaults to `auto`.

            Termite tries entries in order and uses the first available backend+device
            combination that supports the model.

            **Backends** (depend on build tags):
            - `go` - Pure Go inference (always available, CPU only, slowest)
            - `onnx` - ONNX Runtime (requires -tags="onnx,ORT", fastest)
            - `xla` - GoMLX XLA (requires -tags="xla,XLA", TPU/CUDA/CPU)

            **Devices**:
            - `auto` - Auto-detect best available (default)
            - `cuda` - NVIDIA CUDA GPU
            - `coreml` - Apple CoreML (macOS only, used by ONNX)
            - `tpu` - Google TPU (used by XLA)
            - `cpu` - Force CPU only

            **Examples**:
            - `["onnx", "xla", "go"]` - Try backends with auto device detection
            - `["onnx:cuda", "xla:tpu", "onnx:cpu", "go"]` - Prefer GPU, fall back to CPU
            - `["onnx:coreml", "go"]` - macOS with CoreML acceleration
          default:
            - onnx
            - xla
            - go
          example:
            - onnx:cuda
            - xla:tpu
            - onnx:cpu
            - xla:cpu
            - go
        max_concurrent_requests:
          type: integer
          description: |
            Maximum number of concurrent inference requests allowed.
            Additional requests will be queued up to max_queue_size.
            Set to 0 for unlimited (default).
          default: 0
          example: 4
        max_queue_size:
          type: integer
          description: |
            Maximum number of requests to queue when max_concurrent_requests is reached.
            When the queue is full, new requests receive 503 Service Unavailable with Retry-After header.
            Set to 0 for unlimited queue (default). Only effective when max_concurrent_requests > 0.
          default: 0
          example: 100
        request_timeout:
          type: string
          description: |
            Maximum time to wait for a request to complete, including queue wait time.
            Use Go duration format: "30s", "1m", "0" (no timeout, default).
            Requests exceeding this timeout receive 504 Gateway Timeout.
          default: '0'
          example: 30s
        preload:
          type: array
          items:
            type: string
          description: |
            List of model names to preload at startup (Ollama-compatible).
            These models are loaded immediately when Termite starts, avoiding first-request latency.
            Model names should match those in models_dir/embedders/ (e.g., "bge-small-en-v1.5").
            Only effective when keep_alive is non-zero (lazy loading mode).
          example:
            - bge-small-en-v1.5
            - clip-vit-base-patch32
        max_memory_mb:
          type: integer
          description: |
            Maximum memory (in MB) to use for loaded models.
            When this limit is approached, least recently used models are unloaded.
            Set to 0 for unlimited (default). This is an advisory limit - actual memory
            usage depends on model sizes and may temporarily exceed this value.
            Works alongside max_loaded_models for fine-grained control.
          default: 0
          example: 4096
        model_strategies:
          type: object
          additionalProperties:
            type: string
            enum:
              - eager
              - lazy
              - bounded
          description: |
            Per-model loading strategy overrides. Maps model names to their loading strategy.
            Models not in this map use the default strategy based on keep_alive:
            - If keep_alive="0" (default): eager loading (load at startup, never unload)
            - If keep_alive>0: lazy loading (load on demand, unload after idle)

            When a model has strategy "eager" in this map:
            - It is loaded at startup (as part of preload)
            - It is never unloaded, even when keep_alive>0 (pinned in memory)

            This allows mixing eager and lazy models in the same pool.
          example:
            bge-small-en-v1.5: eager
            chonky: lazy
        log:
          $ref: '#/components/schemas/schemas-Config'
    VersionResponse:
      type: object
      required:
        - version
        - git_commit
        - build_time
        - go_version
      properties:
        version:
          type: string
          description: Termite version
          example: v1.0.0
        git_commit:
          type: string
          description: Git commit hash
          example: abc1234
        build_time:
          type: string
          description: Build timestamp
          example: '2024-01-15T10:30:00Z'
        go_version:
          type: string
          description: Go runtime version
          example: go1.25.0
    ContentSecurityConfig:
      type: object
      properties:
        allowed_hosts:
          type: array
          description: Whitelist of allowed hostnames/IPs for link downloads. If empty, all hosts are allowed (except private IPs if block_private_ips is true).
          items:
            type: string
          example:
            - example.com
            - cdn.example.com
            - 192.0.2.1
        block_private_ips:
          type: boolean
          description: Block requests to private IP ranges (127.0.0.0/8, 10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16, 169.254.0.0/16)
          default: true
        max_download_size_bytes:
          type: integer
          description: Maximum size of downloaded content in bytes
          default: 104857600
          minimum: 0
          example: 104857600
          x-go-type: int64
        download_timeout_seconds:
          type: integer
          description: Timeout for individual download operations in seconds
          default: 30
          minimum: 1
          example: 30
        max_image_dimension:
          type: integer
          description: Maximum image width/height in pixels (images will be resized)
          default: 2048
          minimum: 1
          example: 2048
        allowed_paths:
          type: array
          description: Whitelist of allowed path prefixes for file:// and s3:// URLs. If empty, all paths are allowed. For file:// use absolute paths (e.g., /Users/data/). For s3:// use bucket/prefix (e.g., my-bucket/uploads/).
          items:
            type: string
          example:
            - /Users/data/
            - my-bucket/uploads/
    Credentials:
      type: object
      properties:
        endpoint:
          type: string
          description: S3-compatible endpoint (e.g., 's3.amazonaws.com' or 'localhost:9000' for MinIO)
          minLength: 1
          example: s3.amazonaws.com
        use_ssl:
          type: boolean
          description: 'Enable SSL/TLS for S3 connections (default: true for AWS, false for local MinIO)'
          default: true
        access_key_id:
          type: string
          description: AWS access key ID. Supports keystore syntax for secret lookup. Falls back to AWS_ACCESS_KEY_ID environment variable if not set.
          example: your-access-key-id
        secret_access_key:
          type: string
          description: AWS secret access key. Supports keystore syntax for secret lookup. Falls back to AWS_SECRET_ACCESS_KEY environment variable if not set.
          example: your-secret-access-key
        session_token:
          type: string
          description: Optional AWS session token for temporary credentials. Supports keystore syntax for secret lookup.
          example: your-session-token
    Level:
      type: string
      description: Logging verbosity level
      enum:
        - debug
        - info
        - warn
        - error
      default: info
      example: info
    Style:
      type: string
      description: Logging output format style. 'terminal' for colorized console, 'json' for structured JSON, 'logfmt' for token-efficient key=value pairs, 'noop' for silent.
      enum:
        - terminal
        - json
        - logfmt
        - noop
      default: terminal
      example: terminal
    schemas-Config:
      type: object
      description: Logging configuration for Termite services
      properties:
        level:
          $ref: '#/components/schemas/Level'
        style:
          $ref: '#/components/schemas/Style'
